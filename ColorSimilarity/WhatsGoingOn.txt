===== Code Pipeline Walkthrough =====

We have Data backend of 10k images
Test images are random as well, amount dependent on user

=== Idea ===  
    - create quantized color LAB space (~1300 colors) -> LAB space closer to human perception than BGR
    - extract 64 main colors in every image
    - quantize backend images by using ANN(OY) to find nearest quantized color fast in high-dimensional space and map each color to its nearest neighbor
    - vectorize images -> each dimension is a color from quantized color space
    - value of each dimension is equal to amount of pixels representing the specific dimension (each vector has max 64 dims that hold information other than 0)
    - save vectors of backend images 

    - process input-image (extract 64 main colors, map to quantized space, create vector)
    - calculate cosine-similarity with each image-vector from data backend 
    - Pick top 5 images from databse with highest color-similarity

    - Create second quantized LAB space, but with much fewer colors (100 - 150 depending on settings)
    - load images again, extract main colors, map to colors in second LAB space, create vectors
    - Calculate Earth Mover's Distance / Wasserstein-Distance between input image vector (lower dimensional one) and each of top 5 images low-dimensional feature vector
    - Image with smallest Earth Mover's Distance is picked as most similar to input image based on color



=== Code-Steps ===

== Setting up Data backend images (one time thing) (using ImagaData_vector_matrix.py) ==
    - Create quantized lab:
        3 Arrays (each dimension gets one) from values 0 to 255 in stepsize chosen via parameters -> currently: L-channel 8, A- + B-channel each 13 
        Build cartesian product to create tuple of 3 (creates valid color)
        = 1_350 different colors
    
    - Create ANNOYIndex object to later find nearest color in qunatized LAB space
        Build AnnoyIndex, 3 Dimension (one for each color)
        Add each color from  quantized LAB space
        Use 10 trees
    
    - Create feature vector for each image in Data backend  
        get 64 main/dominant colors from image using KMeans
        Find nearest color in AnnoyIndex object for each main/dominant image color and count occurances (if two main colors get mapped to same color then sum up their occurances)
        Place each amount of occurances in feature vector at representing dimension
        Normalize value/vector to L2 (making sure that all vectors have same length, so that we can compare them fairly)
    
    - Stack all the vector and store matrix in .npy file("ImageData_vector_matrix.npy") to avoid same computation all over each time main is executed

== Pipeline to compare input image to images prior stored as vectors in matrix ==
    - create 2 discrete LAB color spaces:
        first one equal to the one that was used creating the backend vectors (1_350 dimns/colors)
        second one with much rougher quantization (5x5x5 = 125 dimns/colors), to be used during EMD calculation
    
    - Prepare 2 AnnoyIndex objects to later map colors to those in quantized LAB-spaces
    
    - load feature-vectors from .npy-file created earlier in first stage 


    Iterate through each image in test_images directory. For each image then:

    - extract 64 main colors, map to those in 1_350 LAB space created earlier before, count occurancies, place them accordingly in vector and L2-normalize said vector
        -> allows for fair comparison of vectors
    
    - Calculate cosine-similarity between each feature vector from backend-image-vectors and input-image-vector
        L2 normalization ensured that 

    - find top 5 images from DataBackend with greatest cosine similarities

    - create new vector for ipnut and top 5 images, this time with second color space (rougher quantization) and L1 normalization (so that values from all dimensions add up 1 -> important to fairly compare images and histograms)

    - calculate EMD between input image and each of top 5 cosine-pickes images from backend. Smallest EMD is most similar image







    
